{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0846408b",
   "metadata": {},
   "source": [
    "# SQL\n",
    "\n",
    "The language you must learn for geospatial analysis!\n",
    "\n",
    "## New\n",
    "\n",
    "SQL is pronounced 'sequel' and is short for structure query language. I'm new to using SQL for geospatial analysis. Still, I am amazed at how easy it is to learn, how fast it analyzes large datasets, and how critical it is for data analysis. SQL is the universal database management language, so geospatial aside, if you work with data, you must learn how to use it. The basic structure of a SQL query is shown in the tips window. Initially, the CREATE and SELECT * FROM commands are used in data exploration, followed by finer-scale querying.\n",
    "\n",
    "```{tip}\n",
    "**BASIC SQL QUERY STRUCTURE**<br>\n",
    "CREATE TABLE IF NOT EXISTS tablename AS <br>\n",
    "SELECT * FROM ('path_to_file') <br>\n",
    "WHERE <br>\n",
    "GROUP BY <br>\n",
    "ORDER BY desc <br>\n",
    "LIMIT\n",
    "```\n",
    "\n",
    "## DuckDB\n",
    "\n",
    "In this chapter, we will use [DuckDB](https://duckdb.org/) as the SQL software package. DuckDB is fast, works seamlessly with many programming languages, including Python, R, and Javascript, and is easy to install. DuckDB also has a spatial extension to perform queries and analyze geospatial data, which we will look at in this chapter. The big advantage of DuckDB is its speed in processing large datasets.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Installation is straightforward: follow the instructions for the command line or Python at the DuckDB installation [page](http://gg.gg/1at9tp).\n",
    "\n",
    "## Wood processing\n",
    "\n",
    "Let's move to an example from the University of California's Woody Biomass Utilization Group's Biomass Power Plant [Database](http://gg.gg/1as7ti). What do mills have to do with forest conservation? We're not talking about rapacious clearcutting from the past but a sustainable forest management model that reduces wildfire and returns western forests to a healthier and more resilient state. Fires threaten most forests in the western United States due to more than a century of fire suppression. \n",
    "\n",
    "Through ecological thinning and prescribed fire, unhealthy forests can return to a natural range of variation by lowering competition across stands. This approach can reduce compounding stresses from drought, insect infestations, and wildfire and create more resilient forests {cite}`north`. Thinning will produce large amounts of forest biomass that can be processed through community-based mills and biomass plants. Developing this infrastructure is one way to help solve the wildfire crisis threatening biodiversity and people.\n",
    "\n",
    "First, import the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a34447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import duckdb\n",
    "import leafmap\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa8859",
   "metadata": {},
   "source": [
    "Then, connect to the duckdb database and install httpfs and spatial extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect database install httpfs, spatial\n",
    "con = duckdb.connect()\n",
    "con.install_extension(\"httpfs\")\n",
    "con.load_extension(\"httpfs\")\n",
    "con.install_extension(\"spatial\")\n",
    "con.load_extension(\"spatial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f60a5",
   "metadata": {},
   "source": [
    "Read the sawmill .shp file and create the sawmill table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sawmill table from shp file and show table\n",
    "con.sql('''\n",
    "    CREATE TABLE IF NOT EXISTS sawmill as\n",
    "    SELECT * FROM ST_Read('./wood.shp')\n",
    "''')\n",
    "con.table('sawmill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd59f8d",
   "metadata": {},
   "source": [
    "Note that ./ reads the sawmill shape file in the CurrentSawmill file if you've cloned the repo to your local computer. If you downloaded a shp file in Windows to your Downloads folder, replace the path to where you've stored the shp files, for example, 'C:/Users/your_user_name/Downloads/shapefilename.shp'.\n",
    "\n",
    "```{tip} Duckdb can be run in the command line and through Python, as we do here. There are several ways to do this, but wrapping the commands in con.sql with parenthesis and two sets of double or single quotes is easier to code and read.\n",
    "```\n",
    "\n",
    "{numref}`sawmill` shows the tabular result after executing that codeblock. The output is truncated by rows and columns so it's often worthwhile viewing the table schema to inform future queries ({numref}`schema`).\n",
    "\n",
    "```{figure} /figures/sql/sawmill.png\n",
    ":height: 500px\n",
    ":name: sawmill\n",
    "The entire sawmill dataset (columns and rows truncated).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sawmill table schema\n",
    "con.sql('''\n",
    "    DESCRIBE sawmill\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd639d8",
   "metadata": {},
   "source": [
    "```{figure} /figures/sql/schema.png\n",
    ":height: 500px\n",
    ":name: schema\n",
    "Sawmill table schema.\n",
    "```\n",
    "Let's run a query that sums sawmills by county ({numref}`county`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum sawmills by county\n",
    "con.sql('''\n",
    "    SELECT County, COUNT(*) as Count\n",
    "    FROM sawmill\n",
    "    GROUP BY County\n",
    "    ORDER BY count DESC\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335e8a2",
   "metadata": {},
   "source": [
    "```{figure} /figures/sql/county.png\n",
    ":height: 500px\n",
    ":name: county\n",
    "Sum of total sawmills by county.\n",
    "```\n",
    "\n",
    "Import the biomass database, then create and show the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62dd174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Biomass dataset\n",
    "con.sql(\"SELECT * FROM ST_Read('./biomass.shp')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9669b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create biomass table from the shp file and show table\n",
    "con.sql('''\n",
    "    CREATE TABLE IF NOT EXISTS biomass as\n",
    "    SELECT * FROM ST_Read('./biomass.shp')\n",
    "''')\n",
    "con.table('biomass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daafb0c",
   "metadata": {},
   "source": [
    "View the biomass table schema, then sum the total megawatts of the plants by county, rounding the total to one decimal ({numref}`mw`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View biomass table schema\n",
    "con.sql('''\n",
    "    DESCRIBE biomass\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b5ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the total MW_Grid by county round to 1 decimal\n",
    "\n",
    "con.sql('''\n",
    "    SELECT County, ROUND(SUM(CAST(MW_Grid AS FLOAT)), 1) as Total_MW_Grid\n",
    "    FROM biomass\n",
    "    GROUP BY County\n",
    "    ORDER BY Total_MW_Grid DESC\n",
    "    LIMIT 15\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05752d",
   "metadata": {},
   "source": [
    "```{figure} /figures/sql/mw.png\n",
    ":height: 500px\n",
    ":name: mw\n",
    "Sum of total megawats of biomass plants by county.\n",
    "```\n",
    "\n",
    "## Lewis's Woodpecker\n",
    "Lewis's Woodpecker (_Melanerpes lewis_) is a striking North American species of woodpecker named after 19th-century explorer Meriwether Lewis ({numref}`lewo`). Although locally common, its breeding and migratory habits are not extensively known. Lewis's Woodpeckers also engage in some un-woodpeckery behaviors such as hawking insects ([Wikipedia](https://en.wikipedia.org/wiki/Lewis%27s_woodpecker)). We'll examine a dataset of Lewis's species occurrence from eBird.\n",
    "\n",
    "```{figure} /figures/sql/lewo.jpg\n",
    ":height: 500px\n",
    ":name: lewo\n",
    "Lewis's Woodpecker (_Melanerpes lewis_). Photo courtesy of wikimedia.org.\n",
    "```\n",
    "\n",
    "Installing the Geo Data Viewer plugin for VS Code will let you quickly view spatial datasets in a kepler.gl viewer. For example, if you cloned the repo, right-clicking on the Lewis's Woodpecker csv file in the test/lewo folder and selecting View Map will give a distribution heatmap ({numref}`lewo_distrib`).\n",
    "\n",
    "```{figure} /figures/sql/lewo_distrib.png\n",
    ":height: 500px\n",
    ":name: lewo_distrib\n",
    "Lewis's Woodpecker distribution heatmap in the western United States.\n",
    "```\n",
    "\n",
    "To analyze the woodpecker data, import the packages and connect to DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167482e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and connect\n",
    "import duckdb\n",
    "import leafmap\n",
    "import pandas as pd\n",
    "con = duckdb.connect()\n",
    "con.install_extension(\"httpfs\")\n",
    "con.load_extension(\"httpfs\")\n",
    "con.install_extension(\"spatial\")\n",
    "con.load_extension(\"spatial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2baae",
   "metadata": {},
   "source": [
    "The Lewis's csv file is quite large, but DuckDB quickly runs through its ~140k rows. Create a table called lewo, short for Lewis's Woodpecker. The csv file is in the repo's test folder. Note using ST_Point to assign the latitude and longitude columns to geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4debd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lewo table and assign points lat lon points to geometry\n",
    "sql = (\"\"\"\n",
    "    CREATE TABLE lewo AS\n",
    "    SELECT *,\n",
    "        ST_Point(LONGITUDE, LATITUDE) as geometry\n",
    "    FROM \"./lewo/lewo.csv\" \n",
    "\"\"\")\n",
    "con.execute(sql)\n",
    "\n",
    "# show the table\n",
    "con.table(\"lewo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6cfb52",
   "metadata": {},
   "source": [
    "If the FROM \"./lew/lewo.csv\" statement throws an error, try replacing the file path by right-clicking the file in your computer and selecting copy path, then passing that file path in place of \"./lewo/low.csv\". For Windows, change the slashes to / instead of \\ for the file path.\n",
    "\n",
    "Then, count the total observations by state ({numref}`lewo_state`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by state\n",
    "con.sql('''\n",
    "    SELECT STATE, COUNT(OBJECTID) as Count\n",
    "    FROM lewo\n",
    "    GROUP BY STATE\n",
    "    ORDER BY Count DESC\n",
    "    LIMIT 10\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe8515",
   "metadata": {},
   "source": [
    "```{figure} /figures/sql/lewo_state.png\n",
    ":height: 500px\n",
    ":name: lewo_state\n",
    "Lewis's Woodpecker counts by state.\n",
    "```\n",
    "\n",
    "Select the Washington observations and name the new table 'wash'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Washington state points and name the table wash\n",
    "con.sql('''\n",
    "    CREATE TABLE wash AS\n",
    "    SELECT *\n",
    "    FROM lewo\n",
    "    WHERE STATE = 'Washington'\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7f514",
   "metadata": {},
   "source": [
    "Export the new table as a csv by converting it to a data frame and save in the test folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data from the wash table into a data frame\n",
    "wash_df = con.execute(\"SELECT * FROM wash\").fetchdf()\n",
    "\n",
    "# Export the dataframe to a csv file in the test folder\n",
    "wash_df.to_csv('test/wash.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155394d",
   "metadata": {},
   "source": [
    "Right-clicking the csv file and selecting View Map produces a heat map. It shows that Lewis's Woodpeckers prefer the Hood River and a certain elevation along the Pacific Crest ({numref}`wash`).\n",
    "\n",
    "```{figure} /figures/sql/wash.png\n",
    ":height: 500px\n",
    ":name: wash\n",
    "Distribution heatmap in Washington state of Lewis's Woodpecker.\n",
    "```\n",
    "\n",
    "Hopefully, these examples are enough to get you started. The resources below have more detailed examples and videos to go deeper. Make sure to practice and try out queries, joins, and other spatial functions with different datasets, including geosjson and geoparquet, to go beyond using shp and csv files.\n",
    "\n",
    "## Resources\n",
    "- **[GEOG-414](https://geog-414.gishub.org/book/duckdb/01_duckdb_intro.html)**. The DuckDB portion of Qiusheng Wu's Geography 414 course is a recommended way to expand your spatial SQL knowledge.\n",
    "- **[Spatial SQL](https://spatial-sql.com/)**. Matt Forrest's text on using SQL in modern GIS is an excellent reference and starter for using SQL within a spatial context. Although the book could use a copyedit (many spelling errors) and better organization (figures disconnected from text, tutorials with more bullets/less text), everything is in the book that you will need to become a spatial SQL expert. The tutorials are relevant and guide you through critical beginner -> advanced workflows using spatial SQL.\n",
    "- **[SQL-QGIS Tip](https://twitter.com/spatialthoughts/status/1774833044396081189)**. You can use the 'Execute SQL' processing algorithm to run SQL queries on ANY vector layer within QGIS. Here's an example of calculating group statistics on a vector layer. This also allows you to run SQL queries in a model.\n",
    "- **[Mark Litwintschik](https://tech.marksblogg.com/duckdb-gis-spatial-extension.html)**. This great data and geospatial blog features several tutorials running the DuckDB spatial extension.\n",
    "- **[Lonboard](https://developmentseed.org/lonboard/latest/)**. Lonboard is a Python library for fast vector processing. The [DuckDB Spatial](https://developmentseed.org/lonboard/latest/examples/duckdb/) tutorial links Lonboard to DuckDB and Python to create a heatmap. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "spatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
